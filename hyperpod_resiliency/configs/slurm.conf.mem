# slurm.conf file generated by AWS

ClusterName=Gpu3

SlurmctldHost=ip-10-4-26-208(10.4.26.208)
SlurmctldParameters=enable_configless

ProctrackType=proctrack/linuxproc
RebootProgram="/usr/bin/systemctl reboot"
ReturnToService=1

# Slurmctld settings
SlurmctldLogFile=/var/log/slurm/slurmctld.log
StateSaveLocation=/var/spool/slurmctld

# Slurmd settings
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmdSpoolDir=/var/spool/slurmd

# Allow dynamic node registration
TreeWidth=65533
MaxNodeCount=32768

SlurmUser=slurm
AuthType=auth/munge

SwitchType=switch/none
TaskPlugin=task/none # Required for auto-resume feature.

# TIMERS
InactiveLimit=0
KillWait=30
MinJobAge=300
SlurmctldTimeout=120
SlurmdTimeout=300
Waittime=0

# SCHEDULING
SchedulerType=sched/backfill
SelectType=select/cons_tres

LaunchParameters=enable_nss_slurm
SchedulerParameters=permit_job_expansion # Required for auto-resume feature.

# ACCOUNTING
Include accounting.conf

# There is no nodes in this cluster yet
PartitionName=dev Nodes=ALL Default=YES MaxTime=INFINITE State=UP OverSubscribe=EXCLUSIVE

NodeName=ip-10-4-69-188 NodeHostname=ip-10-4-69-188 NodeAddr=10.4.69.188 Boards=1 CPUs=4 CoresPerSocket=2 RealMemory=15803 Gres=gpu:1,file:/dev/nvidia0 RealMemory=16384 SocketsPerBoard=1 State=CLOUD ThreadsPerCore=2
NodeName=ip-10-4-94-240 NodeHostname=ip-10-4-94-240 NodeAddr=10.4.94.240 Boards=1 CPUs=4 CoresPerSocket=2 RealMemory=15803 Gres=gpu:1,file:/dev/nvidia0 RealMemory=16384 SocketsPerBoard=1 State=CLOUD ThreadsPerCore=2
NodeName=ip-10-4-5-82 NodeHostname=ip-10-4-5-82 NodeAddr=10.4.5.82 Boards=1 CPUs=4 CoresPerSocket=2 RealMemory=15803 Gres=gpu:1,file:/dev/nvidia0 RealMemory=16384 SocketsPerBoard=1 State=CLOUD ThreadsPerCore=2